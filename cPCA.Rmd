---
title: "PCA, cPCA, occPCA"
author: "Robin Tu"
date: "10/1/2019"
output:
  html_document:
    code_folding: "hide"

---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library("testthat")
library("data.table")
library("ggplot2")
library('Rcpp')
```

### Recap of SVD and some nice properties:

SVD decomposes a $n \times p$ matrix $X = UDV^T$:

  * $U$ is a left unitary $n \times n$ matrix. $U$ is the orthonormal eigenvectors for $XX^T = UDV^T (UDV^T)^T = UD^2U^T$
  * $D$ is a rectangular diagonal $n\times p$ matrix
  * $V^T$ is a right unitary $p\times p$ matrix.  $V$ are the orthonormal eigenvectors for $X^TX=(UDV^T)^TUDV^T = VD^2V^T$.
 
where  Since $U$ and $V$ are unitary, $U^T = U^{-1} \Rightarrow U^TU=  I_{n}$ and $V^T = V^{-1} \Rightarrow  VV^T= V^TV= I_{p}$. 

This decomposition can be thought of as rotation, scaling, and rotation again. SVD also allows you to compute the eigenvectors of the covariance matrix without calculating the covariance matrix and doing an eigen/spectral decomposition on the $p \times p$ matrix. 

**Why can the diagonal elements of $D$ be described as the variance explained by each dimension??**

### Helper Functions:
#### Turning NA's to Zeros.
```{r helper functions}
## convert NA values to zero
NAtoZero = function(mat){
  mat[is.na(mat)] <- 0
  return(mat)
}
```

#### Ploting diagnostics for Principal Components
```{R}
#screeplot for an SVD object
scree = function(svd_obj){
  var_explained <- svd_obj$d^2
  var_explained_plot <- data.table("Components" = 1:length(var_explained), "Value"=cumsum(var_explained)/(sum(var_explained)), "Analysis"="Total Prop. Explained")
  scree_plot <- data.table("Components" = 1:length(var_explained), "Value"=var_explained, "Analysis"="Variance")
  
  plot_dat <- rbind(var_explained_plot, scree_plot)

  ggplot(data = plot_dat)+
    geom_point(aes(x = Components, y = Value))+
    geom_line(aes(x = Components, y = Value), color = "dodgerblue3")+
    facet_grid(Analysis~., scales = "free")+
    ggtitle("Diagnostic Plots")
  }
```

#### Crude Tests 
make sure my helper functions work as they should

```{r test helper function}
test_mat <- matrix(rnorm(1000*4, mean = 100, sd = 20), ncol = 4)

centered_scaled_obj <- scale(test_mat)
scaled_obj <- scale(test_mat, center = F, scale = apply(test_mat, 2, sd, na.rm=T))

# make sure the column means are close to zero (ie less than machine epsilon)
expect_equivalent(apply(centered_scaled_obj, 2, mean),expected = rep(0, 4))
# make sure variances are 1 
expect_equivalent(apply(centered_scaled_obj, 2, var), expected = rep(1, 4))

# make sure the column means do not change
expect_equal(apply(scaled_obj, 2, mean), expected = colMeans(scaled_obj))
# make sure variances are 1 
expect_equivalent(apply(scaled_obj, 2, var), expected = rep(1, 4))


```

### PCA, cPCA, occPCA functions:

#### PCA: Principal Component Analysis

PCA seeks to find the direction $v$ that maximizes the variance/covariance of $Xv$.

\begin{align*}
cov(Xv) &= v^TC_xv\\
\end{align*}

In other words, $v = \text{argmax}_{v}v^TC_x v$ where $C_x$ is the covariance matrix of $X$. This could be found using eigen/spectral decomposition, or using the matrix $V$ from the SVD decomposition, $X = UDV^T$. $V$ gives the eigenvectors corresponding to the rotation explaining the largest variance to the smallest variance.

An example of using PCA on the `iris` dataset:

```{R,  fig.align='center'}
data(iris)
iris <- data.table(iris)
scaled_iris <- scale(iris[, .SD, .SDcols = - "Species"])

# double checking that the results of eigen and svd are the same up to a sign change
expect_equivalent(abs(svd(cov(scaled_iris))$v), abs(eigen(cov(scaled_iris))$vectors))

iris_pca <- cbind((data.matrix(iris[, .SD, .SDcols = -"Species"]) %*% svd(cov(scaled_iris), nv = 2)$v), iris[, .SD, .SDcols ="Species"])
setnames(iris_pca, c("PCA1", "PCA2", "Species"))

ggplot(data = iris_pca)+
  geom_point(aes(x = PCA1, y = PCA2, color = Species))

```

PCA function: 

```{r PCA function}
PCA = function(target, n_components = 2, standardize = T, return_eigenvectors = F){
  if(!is.matrix(target)){
    target <- as.matrix(target)
  }
  og_target <- target
  if(standardize){
    target = scale(target);
  }
  
  target_cov = cov(target);
  v_top <- svd(target_cov, nv = n_components)$v
  reduced_target <- og_target %*% v_top 
  
  if(return_eigenvectors){
    return(list("reduced_target"=reduced_target, "vectors"=v_top))
  }else{
    return(list("reduced_target"=reduced_target))  
  }
}
```

#### cPCA: Contrastive PCA

[Author's written function in Python]("https://github.com/rtud2/contrastive/blob/master/contrastive/__init__.py")

  * Description: Suppose you have two datasets, a `target` and a `background` where the `target` were a dataset containing information on cases and `background` contained information on controls, or uninteresting variation.  cPCA works directly on the covariance matrices and seeks to find the directions/rotation (eigenvector) that maximizes the variance explained in the `target` and minimize the variance explained in the `background`.  More specifically:
  
  \begin{align*}
    \textbf{v}^* &= \text{argmax}_{\textbf{v}\in \mathbb{R^d_{unit}}}{v^T\left(C_{target} - \alpha C_{bg}\right)v}
  \end{align*}
  
  Where 
  
  * Function inputs:
    * target: Target dataset
    * bg: Background dataset
    * n_components: number of Principal components to use
    * alpha: tuning parameter for how hard to subtract the background data
    * return_all: (logical) whether top eigenvectors should be returned
  * Function outputs: 
    * Data projected on the contrastive principal components


```{r cPCA function}
cPCA = function(target, bg, n_components = 2, alpha = 1, standardize = T, return_all = F){
  
  if(!is.matrix(target) | !is.matrix(bg)){
    target <- as.matrix(target)
    bg <- as.matrix(bg)
  }
  og_target <- target
  
  if(standardize){
    target = scale(target);
    bg = scale(bg);
  }
  
  target_cov = cov(target);
  bg_cov = cov(bg);
  
  sigma = target_cov - alpha * bg_cov
  v_top <- svd(sigma, nv = n_components)$v
  reduced_target <- og_target %*% v_top
  reduced_bg <- bg %*% v_top
  
  if(return_all){
    return(list("reduced_target" = reduced_target, "reduced_bg" = reduced_bg, "vectors" = v_top))
  }else{
    return(list("reduced_target" = reduced_target))  
  }
}

```

#### occPCA: Orthogonal Complement cPCA

Procedure:

1. Calculate the Top Principal Components of the background data
2. Compute the projection onto the Top Principal Components of the background data
    + since the eigenvectors, $V_{bg}$ computed by `SVD` are orthonormal, the projection simplifies to $V_{bg}V_{bg}'$
    + this is because $P_{bg} = V_{bg}(V_{bg}'V_{bg})^{-1}V_{bg}'$, but $(V_{bg}'V_{bg})^{-1} = I_{k}$
3. Project the target onto the orthogonal complement of the background, $X_{target} P_{bg}$
4. Find the PCs of the target projected onto the orthogonal complement of the background.
5. Rotate the Target data onto the new PCs

* Function inputs:
  * target: Target dataset
  * bg: Background dataset
  * n_components: number of Principal components to use
  * bg_components: number of background principal components used. Tuning parameter because this affects the Orthogonal Complement 
  * return_all: (logical) whether to return the background PCs and target projected on the Orthogonal Complement of the background
* Function outputs: 
  * Data projected on the Orthogonal Complement contrastive principal components


```{R, fig.align='center'}
occPCA = function(target, bg, n_components = 2, bg_components = 2, standardize = T, return_all = F){
  if(!is.matrix(target) | !is.matrix(bg)){
    target <- as.matrix(target)
    bg <- as.matrix(bg)
  }
  og_target <- target
  
  if(standardize){
    target = scale(target);
    bg = scale(bg);
  }
  # Rotate the background
  bg_svd <-svd(bg, nv = bg_components)
  
  # since bg_svd$v is an orthonormal basis, the projection matrix is V %*% t(V)
  # a simple test: 
  # bg_space <- bg %*% bg_svd$v 
  # expect_equivalent(bg_space %*% tcrossprod(bg_svd$v) -  bg_space, matrix(0, nrow =nrow(bg_space), ncol=ncol(bg_space)))
  
  bg_projection <- tcrossprod(bg_svd$v)
  
  #projection onto the orthogonal complement
  oc_target <-  target %*% (diag(nrow = nrow(bg_projection)) - bg_projection)
  
  reduced_target <- og_target %*% svd(oc_target, nv = n_components)$v
 
  if(return_all){
    return(list("reduced_target" = reduced_target, "bg_svd" = bg_svd, "oc_target" = oc_target))
  }else{
    return(list("reduced_target" = reduced_target))  
  }
}


```

### Replicating Figure 3a. in the paper

[Authors Data Analysis]("https://github.com/rtud2/contrastive/blob/master/experiments/Mice%20Protein%20(Figure%202).ipynb?short_path=b1a901a")

The original data had some missing data in it. From the Author's anaylsis, missing values were turned to zero.

```{r}
mouse <- fread('../contrastive/experiments/datasets/Data_Cortex_Nuclear.csv')
mouse <- NAtoZero(mouse)

targ <- mouse[Behavior == "S/C" & Treatment == "Saline" & Genotype %in% c("Control", "Ts65Dn"), .SD, .SDcols = -c("MouseID","class")]
background <- mouse[Behavior == "S/C" & Treatment == "Saline" & Genotype == "Ts65Dn", .SD, .SDcols = -c("MouseID","class")]
```


```{r, fig.align='center'}
results_cPCA <- cPCA(target = targ[, .SD, .SDcols = -c("Genotype", "Treatment", "Behavior")],
     bg = background[, .SD, .SDcols = -c("Genotype", "Treatment", "Behavior")],
     alpha = 1)

reduced_target <- data.table(results_cPCA[[1]])
reduced_target <- cbind(reduced_target,  targ[, .SD, .SDcols = c("Genotype")], "Contrastive")

normal_pca <- data.table(PCA(target = targ[, .SD, .SDcols = -c("Genotype", "Treatment", "Behavior")])[[1]])
normal_pca <- cbind(normal_pca, targ[, .SD, .SDcols = c("Genotype")], "Normal")

first_pass_plot <- rbind(reduced_target, normal_pca)
setnames(first_pass_plot, c("PCA.1", "PCA.2", "Down.Syndrome", "Method"))

ggplot(data = first_pass_plot)+
  geom_point(aes(x = PCA.1, y=PCA.2, color = Down.Syndrome))+
  facet_wrap(~Method, scale = "free")+
  theme(legend.position = "bottom")

```

### occPCA
```{R,  fig.align='center'}
results_occPCA <- occPCA(target = targ[, .SD, .SDcols = -c("Genotype", "Treatment", "Behavior")],
     bg = background[, .SD, .SDcols = -c("Genotype", "Treatment", "Behavior")],
     bg_components = 3,
     return_all = T)

occ_reduced_target <- data.table(results_occPCA[[1]])
occ_reduced_target <- cbind(occ_reduced_target,  targ[, .SD, .SDcols = c("Genotype")], "OCC")
setnames(occ_reduced_target, c("PCA.1", "PCA.2", "Down.Syndrome", "Method"))

second_pass_plot <- rbind(first_pass_plot[Method == "Contrastive"], occ_reduced_target)
```


```{R,  fig.align='center'}
#screeplot
scree(results_occPCA$bg_svd)

ggplot(data = second_pass_plot)+
  geom_point(aes(x = PCA.1, y=PCA.2, color = Down.Syndrome))+
  facet_wrap(~Method, scale = "free")+
  theme(legend.position = "bottom")

```

### Playing with the tuning parameter alpha
Contrastive PCA seems very sensitive to the tuning parameter $\alpha$

```{r,  fig.align='center'}
search_grid <- seq(0, 2.25, by = 0.25)

tuning_alpha <- lapply(search_grid, function(xx) cPCA(target = targ[, .SD, .SDcols = -c("Genotype", "Treatment", "Behavior")],
     bg = background[, .SD, .SDcols = -c("Genotype", "Treatment", "Behavior")],
     alpha = xx))

tuning_alpha_plot <- data.table(do.call(rbind, lapply(tuning_alpha, "[[", 1)),
                                sort(rep(search_grid, nrow(targ))),
                                unlist(rep(targ[, "Genotype"],length(search_grid))))
setnames(tuning_alpha_plot, c("PCA.1", "PCA.2", "alpha", "Down.Syndrome"))

ggplot(data = tuning_alpha_plot)+
  geom_point(aes(x = PCA.1, y = PCA.2, color = Down.Syndrome), alpha = 0.7)+
  facet_wrap(~alpha, scales = "free", nrow = 2)+
  theme(legend.position = "bottom")+
  labs(title = "Mouse Down Syndrome Data: Contrastive PCA with different alpha")

```


Choosing $\alpha$ using log-spacing and spectral clustering 
```{R}
search_grid2 <- c(0, 10^(seq(-1, log10(1000), length.out = 9)))
```
